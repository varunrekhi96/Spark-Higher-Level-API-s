{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18260b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    " builder. \\\n",
    " config('spark.ui.port', '0'). \\\n",
    " config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    " enableHiveSupport(). \\\n",
    " master('yarn'). \\\n",
    " getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2748c4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:41551\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4a6371b400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046d4c",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff83730",
   "metadata": {},
   "source": [
    "#### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd6ee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec61f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|namespace                   |\n",
      "+----------------------------+\n",
      "|trendytech_assignments_week5|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace ='trendytech_assignments_week5'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6dacfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b600a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a2fc348",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:112)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3352)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1210)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:1041)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1667)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:165)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:112)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3352)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1210)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:1041)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-88f979cfa28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table trendytech_assignments_week5.groceries(order_id string,location string,item string,order_date string,quantity integer)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:112)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3352)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1210)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:1041)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1667)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:165)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:112)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3352)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1210)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:1041)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table trendytech_assignments_week5.groceries(order_id string,location string,item string,order_date string,quantity integer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee65150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec01e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df = spark.read.csv(\"/public/trendytech/groceries.csv\",header=\"true\",inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0045a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groceries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f07b5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+----------+--------+\n",
      "|order_id|location|    item|order_date|quantity|\n",
      "+--------+--------+--------+----------+--------+\n",
      "|      o1| Seattle| Bananas|01/01/2017|       7|\n",
      "|      o2|    Kent|  Apples|02/01/2017|      20|\n",
      "|      o3|Bellevue| Flowers|02/01/2017|      10|\n",
      "|      o4| Redmond|    Meat|03/01/2017|      40|\n",
      "|      o5| Seattle|Potatoes|04/01/2017|       9|\n",
      "+--------+--------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groceries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e00d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.createOrReplaceTempView(\"groceries_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ac6ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+--------+\n",
      "|order_id| location|    item|order_date|quantity|\n",
      "+--------+---------+--------+----------+--------+\n",
      "|      o1|  Seattle| Bananas|01/01/2017|       7|\n",
      "|      o2|     Kent|  Apples|02/01/2017|      20|\n",
      "|      o3| Bellevue| Flowers|02/01/2017|      10|\n",
      "|      o4|  Redmond|    Meat|03/01/2017|      40|\n",
      "|      o5|  Seattle|Potatoes|04/01/2017|       9|\n",
      "|      o6| Bellevue|   Bread|04/01/2017|       5|\n",
      "|      o7|  Redmond|   Bread|05/01/2017|       5|\n",
      "|      o8| Issaquah|   Onion|05/01/2017|       4|\n",
      "|      o9|  Redmond|  Cheese|05/01/2017|      15|\n",
      "|     o10| Issaquah|   Onion|06/01/2017|       4|\n",
      "|     o11|   Renton|   Bread|05/01/2017|       5|\n",
      "|     o12| Issaquah|   Onion|07/01/2017|       4|\n",
      "|     o13|Sammamish|   Bread|07/01/2017|       5|\n",
      "|     o14| Issaquah|  Tomato|07/01/2017|       6|\n",
      "|     o15| Issaquah|    Meat|08/01/2017|       3|\n",
      "|     o16| Issaquah|    Meat|09/01/2017|       5|\n",
      "|     o17| Issaquah|    Meat|10/01/2017|       6|\n",
      "|     o18| Bellevue|   Bread|11/01/2017|       7|\n",
      "|     o19| Bellevue|   Bread|12/01/2017|      54|\n",
      "|     o20| Bellevue|   Bread|13/01/2017|      34|\n",
      "+--------+---------+--------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from groceries_temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30d06f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table not found: trendytech_assignments_week5.groceries;;\n'InsertIntoStatement 'UnresolvedRelation [trendytech_assignments_week5, groceries], false, false\n+- Project [order_id#150, location#151, item#152, order_date#153, quantity#154]\n   +- SubqueryAlias groceries_temp\n      +- Relation[order_id#150,location#151,item#152,order_date#153,quantity#154] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bfa9ad0f42ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insert into trendytech_assignments_week5.groceries select * from groceries_temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table not found: trendytech_assignments_week5.groceries;;\n'InsertIntoStatement 'UnresolvedRelation [trendytech_assignments_week5, groceries], false, false\n+- Project [order_id#150, location#151, item#152, order_date#153, quantity#154]\n   +- SubqueryAlias groceries_temp\n      +- Relation[order_id#150,location#151,item#152,order_date#153,quantity#154] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert into trendytech_assignments_week5.groceries select * from groceries_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60013d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.groceries limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf181afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe66bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended trendytech_assignments_week5.groceries\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374825f1",
   "metadata": {},
   "source": [
    "#### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2e1c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1130)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:724)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1608)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:952)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:949)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:959)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:507)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:410)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:275)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:246)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:326)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:119)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1130)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:724)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:637)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1606)\n\t... 41 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a490a59113dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table trendytech_assignments_week5.groceries_external_table(order_id string,location string,item string,order_date string,quantity integer) using csv options(header='true') location '/public/trendytech/groceries.csv'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1130)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:724)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1608)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:952)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:949)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:959)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:507)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:410)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:275)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:246)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:326)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:119)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv006334, access=EXECUTE, inode=\"/user/itv006753\":itv006753:supergroup:drwx------\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:412)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:703)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1858)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1876)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:718)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1130)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:724)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:637)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1606)\n\t... 41 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table trendytech_assignments_week5.groceries_external_table(order_id string,location string,item string,order_date string,quantity integer) using csv options(header='true') location '/public/trendytech/groceries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e414b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.groceries_external_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended trendytech_assignments_week5.groceries_external_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abacb9b",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226edbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.groceries limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8947a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.groceries_external_table limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e1358",
   "metadata": {},
   "source": [
    "#### 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4b993",
   "metadata": {},
   "source": [
    "#### Dropping external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table trendytech_assignments_week5.groceries_external_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec394ac2",
   "metadata": {},
   "source": [
    "#### Dropping Managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table trendytech_assignments_week5.groceries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b648ffd",
   "metadata": {},
   "source": [
    "#### 1.5  #Performing all the above tasks with the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7de958",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use trendytech_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade56c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create table trendytech_assignments_week5.orders(order_id integer,order_date string,customer_id integer,order_status string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.json(\"/public/trendytech/datasets/orders.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26779d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_new = orders_df.select(\"order_id\", \"order_date\", \"customer_id\",\"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7514738",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_new.createOrReplaceTempView(\"orders_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ed19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from orders_temp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbdab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"insert into trendytech_assignments_week5.orders select * from\n",
    "orders_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.orders limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b048b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00962ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended trendytech_assignments_week5.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create table trendytech_assignments_week5.orders_external(order_id integer,order_date string,customer_id integer,order_status string) using json location '/public/trendytech/datasets/orders.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended trendytech_assignments_week5.orders_external\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.orders_external limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from trendytech_assignments_week5.orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305d166",
   "metadata": {},
   "source": [
    "#### Dropping external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75287c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table trendytech_assignments_week5.orders_external\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe96c84",
   "metadata": {},
   "source": [
    "#### Dropping Managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4510abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table trendytech_assignments_week5.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f554d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710434",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca230ba1",
   "metadata": {},
   "source": [
    "##### Performing the tasks using Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe304de",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df=spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load(\"/user/itv006334/products_folder/products_wh_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05163c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------------------------------------+-----------+------+-------------------------------------------------------------------------------------+\n",
      "|product_id|category|product_name                                 |description|price |image_url                                                                            |\n",
      "+----------+--------+---------------------------------------------+-----------+------+-------------------------------------------------------------------------------------+\n",
      "|1         |2       |Quest Q64 10 FT. x 10 FT. Slant Leg Instant U|null       |59.98 |http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy|\n",
      "|2         |2       |Under Armour Men's Highlight MC Football Clea|null       |129.99|http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat     |\n",
      "|3         |2       |Under Armour Men's Renegade D Mid Football Cl|null       |89.99 |http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat   |\n",
      "|4         |2       |Under Armour Men's Renegade D Mid Football Cl|null       |89.99 |http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat   |\n",
      "|5         |2       |Riddell Youth Revolution Speed Custom Footbal|null       |199.99|http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet|\n",
      "+----------+--------+---------------------------------------------+-----------+------+-------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6834a",
   "metadata": {},
   "source": [
    "##### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2810f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345\n"
     ]
    }
   ],
   "source": [
    "total_products = products_df.count()\n",
    "print(total_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4cec2",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc591909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories:  55\n"
     ]
    }
   ],
   "source": [
    "unique_categories = products_df.select(\"Category\").distinct().count()\n",
    "print(\"Number of unique categories: \", unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e822c4",
   "metadata": {},
   "source": [
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c8639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "|product_name                                    |category|price  |image_url                                                                          |\n",
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "|SOLE E35 Elliptical                             |10      |1999.99|http://images.acmesports.sports/SOLE+E35+Elliptical                                |\n",
      "|SOLE F85 Treadmill                              |4       |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|SOLE F85 Treadmill                              |10      |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|SOLE F85 Treadmill                              |22      |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|\"Spalding Beast 60\"\" Glass Portable Basketball \"|47      |1099.99|http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop|\n",
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_expensive_products = products_df.select(\"product_name\", \"category\", \"price\", \"image_url\").orderBy(\"price\", ascending = False).limit(5)\n",
    "top5_expensive_products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316002",
   "metadata": {},
   "source": [
    "#### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f75334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|Category|Number_of_products|\n",
      "+--------+------------------+\n",
      "|31      |17                |\n",
      "|53      |16                |\n",
      "|34      |15                |\n",
      "|44      |9                 |\n",
      "|12      |3                 |\n",
      "|22      |4                 |\n",
      "|47      |10                |\n",
      "|52      |5                 |\n",
      "|13      |1                 |\n",
      "|6       |5                 |\n",
      "|16      |11                |\n",
      "|3       |5                 |\n",
      "|20      |7                 |\n",
      "|57      |6                 |\n",
      "|54      |6                 |\n",
      "|48      |17                |\n",
      "|5       |11                |\n",
      "|19      |13                |\n",
      "|41      |11                |\n",
      "|43      |23                |\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_above_100 = products_df.filter(\"price > 100\") \\\n",
    ".groupBy(\"Category\") \\\n",
    ".count() \\\n",
    ".withColumnRenamed(\"count\", \"Number_of_products\")\n",
    "products_above_100.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02366d",
   "metadata": {},
   "source": [
    "#### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0757fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+------+\n",
      "|product_name                                    |price |\n",
      "+------------------------------------------------+------+\n",
      "|\"Goaliath 54\"\" In-Ground Basketball Hoop with P\"|499.99|\n",
      "|Fitness Gear 300 lb Olympic Weight Set          |209.99|\n",
      "|Teeter Hang Ups NXT-S Inversion Table           |299.99|\n",
      "+------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expensive_cat_products = products_df.filter(\"price > 200 and category = 5\")\n",
    "result = expensive_cat_products.select(\"product_name\", \"price\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe9a0d",
   "metadata": {},
   "source": [
    "#### Performing the tasks using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef951c",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c56744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|total_products|\n",
      "+--------------+\n",
      "|          1345|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.createOrReplaceTempView(\"products\")\n",
    "total_products = spark.sql(\"select COUNT(*) as total_products from products\")\n",
    "total_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562a95c",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef59238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|unique_categories|\n",
      "+-----------------+\n",
      "|               55|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_categories = spark.sql(\"select COUNT(DISTINCT category) as unique_categories from products\")\n",
    "unique_categories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07bafb",
   "metadata": {},
   "source": [
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8db1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "|product_name                                    |category|price  |image_url                                                                          |\n",
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "|SOLE E35 Elliptical                             |10      |1999.99|http://images.acmesports.sports/SOLE+E35+Elliptical                                |\n",
      "|SOLE F85 Treadmill                              |4       |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|SOLE F85 Treadmill                              |10      |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|SOLE F85 Treadmill                              |22      |1799.99|http://images.acmesports.sports/SOLE+F85+Treadmill                                 |\n",
      "|\"Spalding Beast 60\"\" Glass Portable Basketball \"|47      |1099.99|http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop|\n",
      "+------------------------------------------------+--------+-------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_5_expensive_products = spark.sql(\"select product_name,category,price,image_url from products ORDER BY Price DESC LIMIT 5\")\n",
    "top_5_expensive_products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ce270",
   "metadata": {},
   "source": [
    "#### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98e118c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_gt_100 = spark.sql(\"SELECT category,COUNT(*) AS product_count FROM products WHERE price > 100 GROUP BY category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faca209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|category|product_count|\n",
      "+--------+-------------+\n",
      "|      31|           17|\n",
      "|      53|           16|\n",
      "|      34|           15|\n",
      "|      44|            9|\n",
      "|      12|            3|\n",
      "|      22|            4|\n",
      "|      47|           10|\n",
      "|      52|            5|\n",
      "|      13|            1|\n",
      "|       6|            5|\n",
      "|      16|           11|\n",
      "|       3|            5|\n",
      "|      20|            7|\n",
      "|      57|            6|\n",
      "|      54|            6|\n",
      "|      48|           17|\n",
      "|       5|           11|\n",
      "|      19|           13|\n",
      "|      41|           11|\n",
      "|      43|           23|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_gt_100.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0149bd",
   "metadata": {},
   "source": [
    "#### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07002a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+------+\n",
      "|product_name                                    |price |\n",
      "+------------------------------------------------+------+\n",
      "|\"Goaliath 54\"\" In-Ground Basketball Hoop with P\"|499.99|\n",
      "|Fitness Gear 300 lb Olympic Weight Set          |209.99|\n",
      "|Teeter Hang Ups NXT-S Inversion Table           |299.99|\n",
      "+------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_gt_200 = spark.sql(\"select product_name,price from products WHERE Price > 200 AND category = 5\")\n",
    "products_gt_200.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d4cfe",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab739b7e",
   "metadata": {},
   "source": [
    "#### Performing the tasks using Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c6d211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_email|cust_password|         cust_street|  cust_city|cust_state|cust_zipcode|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "|   null|cust_fname|cust_lname|cust_email|cust_password|         cust_street|  cust_city|cust_state|        null|\n",
      "|      1|   Richard| Hernandez| XXXXXXXXX|    XXXXXXXXX|  6303 Heather Plaza|Brownsville|        TX|       78521|\n",
      "|      2|      Mary|   Barrett| XXXXXXXXX|    XXXXXXXXX|9526 Noble Embers...|  Littleton|        CO|       80126|\n",
      "|      3|       Ann|     Smith| XXXXXXXXX|    XXXXXXXXX|3422 Blue Pioneer...|     Caguas|        PR|         725|\n",
      "|      4|      Mary|     Jones| XXXXXXXXX|    XXXXXXXXX|  8324 Little Common| San Marcos|        CA|       92069|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df=spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load(\"/user/itv006334/customer_data/customer_wh_data.csv\")\n",
    "cust_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddf83e",
   "metadata": {},
   "source": [
    "#### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab4c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|cust_state|count|\n",
      "+----------+-----+\n",
      "|        AL|    3|\n",
      "|        AR|   12|\n",
      "|        AZ|  213|\n",
      "|        CA| 2012|\n",
      "|        CO|  122|\n",
      "|        CT|   73|\n",
      "|        DC|   42|\n",
      "|        DE|   23|\n",
      "|        FL|  374|\n",
      "|        GA|  169|\n",
      "|        HI|   87|\n",
      "|        IA|    5|\n",
      "|        ID|    9|\n",
      "|        IL|  523|\n",
      "|        IN|   40|\n",
      "|        KS|   29|\n",
      "|        KY|   35|\n",
      "|        LA|   63|\n",
      "|        MA|  113|\n",
      "|        MD|  164|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_counts = cust_df.groupBy(\"cust_state\").count().orderBy(\"cust_state\")\n",
    "state_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8b535",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc02c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|cust_lname|count|\n",
      "+----------+-----+\n",
      "|     Smith| 4626|\n",
      "|   Johnson|   76|\n",
      "|  Williams|   69|\n",
      "|     Jones|   65|\n",
      "|     Brown|   62|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_name_counts = cust_df.groupBy(\"cust_lname\").count().orderBy(\"count\",ascending=False).limit(5)\n",
    "last_name_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc38ba",
   "metadata": {},
   "source": [
    "#### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85580442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are customers with invalid zip codes:\n",
      "+-------+----------+----------+----------+-------------+--------------------+-------------+----------+------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_email|cust_password|         cust_street|    cust_city|cust_state|cust_zipcode|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-------------+----------+------------+\n",
      "|      3|       Ann|     Smith| XXXXXXXXX|    XXXXXXXXX|3422 Blue Pioneer...|       Caguas|        PR|         725|\n",
      "|      5|    Robert|    Hudson| XXXXXXXXX|    XXXXXXXXX|10 Crystal River ...|       Caguas|        PR|         725|\n",
      "|      6|      Mary|     Smith| XXXXXXXXX|    XXXXXXXXX|3151 Sleepy Quail...|      Passaic|        NJ|        7055|\n",
      "|      7|   Melissa|    Wilcox| XXXXXXXXX|    XXXXXXXXX|9453 High Concession|       Caguas|        PR|         725|\n",
      "|      8|     Megan|     Smith| XXXXXXXXX|    XXXXXXXXX|3047 Foggy Forest...|     Lawrence|        MA|        1841|\n",
      "|      9|      Mary|     Perez| XXXXXXXXX|    XXXXXXXXX| 3616 Quaking Street|       Caguas|        PR|         725|\n",
      "|     11|      Mary|   Huffman| XXXXXXXXX|    XXXXXXXXX|    3169 Stony Woods|       Caguas|        PR|         725|\n",
      "|     13|      Mary|   Baldwin| XXXXXXXXX|    XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|        PR|         725|\n",
      "|     16|   Tiffany|     Smith| XXXXXXXXX|    XXXXXXXXX|      6651 Iron Port|       Caguas|        PR|         725|\n",
      "|     19| Stephanie|  Mitchell| XXXXXXXXX|    XXXXXXXXX|3543 Red Treasure...|       Caguas|        PR|         725|\n",
      "|     20|      Mary|     Ellis| XXXXXXXXX|    XXXXXXXXX|      4703 Old Route|West New York|        NJ|        7093|\n",
      "|     21|   William| Zimmerman| XXXXXXXXX|    XXXXXXXXX|3323 Old Willow M...|       Caguas|        PR|         725|\n",
      "|     22|    Joseph|     Smith| XXXXXXXXX|    XXXXXXXXX|7740 Broad Fox Vi...| North Bergen|        NJ|        7047|\n",
      "|     23|  Benjamin|    Duarte| XXXXXXXXX|    XXXXXXXXX|8811 High Horse I...|     San Juan|        PR|         921|\n",
      "|     24|      Mary|     Smith| XXXXXXXXX|    XXXXXXXXX| 9417 Emerald Towers|       Caguas|        PR|         725|\n",
      "|     27|      Mary|   Vincent| XXXXXXXXX|    XXXXXXXXX|1768 Sleepy Zephy...|       Caguas|        PR|         725|\n",
      "|     30|   Barbara|     Smith| XXXXXXXXX|    XXXXXXXXX|   2455 Merry Hollow|       Caguas|        PR|         725|\n",
      "|     32|     Alice|     Smith| XXXXXXXXX|    XXXXXXXXX|   2082 Hidden Green|       Caguas|        PR|         725|\n",
      "|     34|      Mary|     Smith| XXXXXXXXX|    XXXXXXXXX|3330 Easy Berry R...|       Caguas|        PR|         725|\n",
      "|     36|  Michelle|     Carey| XXXXXXXXX|    XXXXXXXXX| 6336 Fallen Village|       Caguas|        PR|         725|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "invalid_zips = cust_df.filter(length(\"cust_zipcode\") != 5)\n",
    "if invalid_zips.count() == 0:\n",
    "    print(\"All customers have valid zip codes\")\n",
    "else:\n",
    "    print(\"There are customers with invalid zip codes:\")\n",
    "\n",
    "invalid_zips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20df162",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51df6a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7244\n"
     ]
    }
   ],
   "source": [
    "valid_zips = cust_df.filter(length(\"cust_zipcode\") == 5).count()\n",
    "print(valid_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea9774",
   "metadata": {},
   "source": [
    "#### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c6fcdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    cust_city|count|\n",
      "+-------------+-----+\n",
      "|       Corona|   14|\n",
      "|    Pittsburg|    4|\n",
      "|      Compton|   19|\n",
      "|    Palo Alto|    6|\n",
      "|      Hanford|    9|\n",
      "|      Anaheim|   19|\n",
      "|       Folsom|    6|\n",
      "|         Napa|    8|\n",
      "|     Temecula|    6|\n",
      "|       Reseda|    6|\n",
      "|    Encinitas|   17|\n",
      "|    Oceanside|   24|\n",
      "|    Cupertino|    9|\n",
      "|      Oakland|    3|\n",
      "|        Davis|    9|\n",
      "|      Fontana|   18|\n",
      "|Mission Viejo|   26|\n",
      "|       Madera|    5|\n",
      "|    Elk Grove|   10|\n",
      "|  Bakersfield|   41|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_customers = cust_df.filter(\"cust_state == 'CA'\")\n",
    "ca_customer_counts = ca_customers.groupBy(\"cust_city\").count()\n",
    "print(ca_customer_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef9a82b",
   "metadata": {},
   "source": [
    "#### Performing the tasks using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ddae5",
   "metadata": {},
   "source": [
    "#### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1b712bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|cust_state|customer_count|\n",
      "+----------+--------------+\n",
      "|        AZ|           213|\n",
      "|        SC|            41|\n",
      "|        LA|            63|\n",
      "|        MN|            39|\n",
      "|        NJ|           219|\n",
      "|        DC|            42|\n",
      "|        OR|           119|\n",
      "|        VA|           136|\n",
      "|        RI|            15|\n",
      "|        KY|            35|\n",
      "|        MI|           254|\n",
      "|        NV|           103|\n",
      "|        WI|            64|\n",
      "|        ID|             9|\n",
      "|        CA|          2012|\n",
      "|        CT|            73|\n",
      "|        MT|             7|\n",
      "|        NC|           150|\n",
      "|        MD|           164|\n",
      "|        DE|            23|\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.createOrReplaceTempView(\"customers\")\n",
    "customers_per_state = spark.sql(\"select cust_state,COUNT(*) AS customer_count FROM customers GROUP BY cust_state\")\n",
    "customers_per_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634f75b",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6adae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|cust_lname|last_name_count|\n",
      "+----------+---------------+\n",
      "|     Smith|           4626|\n",
      "|   Johnson|             76|\n",
      "|  Williams|             69|\n",
      "|     Jones|             65|\n",
      "|     Brown|             62|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_5_last_names = spark.sql(\"select cust_lname,COUNT(*) AS last_name_count FROM customers GROUP BY cust_lname ORDER BY last_name_count DESC LIMIT 5\")\n",
    "top_5_last_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb31df",
   "metadata": {},
   "source": [
    "#### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9533d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-------------+--------------------+---------+----------+------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_email|cust_password|         cust_street|cust_city|cust_state|cust_zipcode|\n",
      "+-------+----------+----------+----------+-------------+--------------------+---------+----------+------------+\n",
      "|      3|       Ann|     Smith| XXXXXXXXX|    XXXXXXXXX|3422 Blue Pioneer...|   Caguas|        PR|         725|\n",
      "|      5|    Robert|    Hudson| XXXXXXXXX|    XXXXXXXXX|10 Crystal River ...|   Caguas|        PR|         725|\n",
      "|      6|      Mary|     Smith| XXXXXXXXX|    XXXXXXXXX|3151 Sleepy Quail...|  Passaic|        NJ|        7055|\n",
      "|      7|   Melissa|    Wilcox| XXXXXXXXX|    XXXXXXXXX|9453 High Concession|   Caguas|        PR|         725|\n",
      "|      8|     Megan|     Smith| XXXXXXXXX|    XXXXXXXXX|3047 Foggy Forest...| Lawrence|        MA|        1841|\n",
      "|      9|      Mary|     Perez| XXXXXXXXX|    XXXXXXXXX| 3616 Quaking Street|   Caguas|        PR|         725|\n",
      "+-------+----------+----------+----------+-------------+--------------------+---------+----------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invalid_zips = spark.sql(\"SELECT * FROM customers WHERE LENGTH(cust_zipcode) != 5\")\n",
    "invalid_zips.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752fb9d",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f98c3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|valid_zipcodes_count|\n",
      "+--------------------+\n",
      "|                7244|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_zipcodes_count = spark.sql(\"SELECT COUNT(*) AS valid_zipcodes_count FROM customers WHERE LENGTH(cust_zipcode) == 5\")\n",
    "valid_zipcodes_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc419db5",
   "metadata": {},
   "source": [
    "#### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea5d28c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|    cust_city|customer_count|\n",
      "+-------------+--------------+\n",
      "|       Corona|            14|\n",
      "|    Pittsburg|             4|\n",
      "|      Compton|            19|\n",
      "|    Palo Alto|             6|\n",
      "|      Hanford|             9|\n",
      "|      Anaheim|            19|\n",
      "|       Folsom|             6|\n",
      "|         Napa|             8|\n",
      "|     Temecula|             6|\n",
      "|       Reseda|             6|\n",
      "|    Encinitas|            17|\n",
      "|    Oceanside|            24|\n",
      "|    Cupertino|             9|\n",
      "|      Oakland|             3|\n",
      "|        Davis|             9|\n",
      "|      Fontana|            18|\n",
      "|Mission Viejo|            26|\n",
      "|       Madera|             5|\n",
      "|    Elk Grove|            10|\n",
      "|  Bakersfield|            41|\n",
      "+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "california_customers_per_city = spark.sql(\"select cust_city, COUNT(*) AS customer_count FROM customers WHERE cust_state = 'CA' GROUP BY cust_city\")\n",
    "california_customers_per_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b204e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
